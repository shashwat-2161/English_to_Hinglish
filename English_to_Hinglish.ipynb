{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Required Libraries"
      ],
      "metadata": {
        "id": "3hxfGvWmk0DM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "import string"
      ],
      "metadata": {
        "id": "Tb2FyaEMQ5vm"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imorting google drive to acess dataset from drive\n",
        "\n",
        "Note - you can skip this step if you have data locally.\n"
      ],
      "metadata": {
        "id": "2ANmxCVXlCLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ueFhSzQRBct",
        "outputId": "bd69c953-8ffb-4645-8dc0-01cf0f50f53d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading Dataset"
      ],
      "metadata": {
        "id": "v8jI1R8EldeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your dataset from the CSV file\n",
        "df = pd.read_csv('drive/MyDrive/Assignment_3/Train.csv')"
      ],
      "metadata": {
        "id": "HVDofeLLRqvB"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extarting and preprocessing data"
      ],
      "metadata": {
        "id": "Z3txuXD5llrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the English and Hinglish sentences from the DataFrame\n",
        "english_sentences = df['english_sentences'][:750].tolist()\n",
        "hinglish_sentences = df['hinglish_sentences'][:750].tolist()\n",
        "english_sentences = [''.join(char for char in item if char not in string.punctuation)for item in english_sentences]\n",
        "hinglish_sentences = [''.join(char for char in item if char not in string.punctuation)for item in hinglish_sentences]"
      ],
      "metadata": {
        "id": "FdtMkTNVRDjL"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating token by converting the text to vector formate"
      ],
      "metadata": {
        "id": "-di4-6mKIOOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "english_tokenizer = keras.layers.TextVectorization(output_mode='int')\n",
        "english_tokenizer.adapt(english_sentences)\n",
        "\n",
        "hinglish_tokenizer = keras.layers.TextVectorization(output_mode='int')\n",
        "hinglish_tokenizer.adapt(hinglish_sentences)"
      ],
      "metadata": {
        "id": "LbTr2v97A1Xg"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Defining Encoder and Decoder"
      ],
      "metadata": {
        "id": "OVwVU_MlIfS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the encoder-decoder model\n",
        "input_layer = Input(shape=(None,))\n",
        "encoder = keras.layers.Embedding(input_dim=len(english_tokenizer.get_vocabulary()), output_dim=256)(input_layer)\n",
        "encoder, state_h, state_c = LSTM(256, return_state=True)(encoder)\n",
        "\n",
        "decoder_input = Input(shape=(None,))\n",
        "decoder = keras.layers.Embedding(input_dim=len(hinglish_tokenizer.get_vocabulary()), output_dim=256)(decoder_input)\n",
        "decoder, _, _ = LSTM(256, return_sequences=True, return_state=True)(decoder, initial_state=[state_h, state_c])\n",
        "output = Dense(len(hinglish_tokenizer.get_vocabulary()), activation='softmax')(decoder)"
      ],
      "metadata": {
        "id": "20DWWEydA43R"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating Models and training Models"
      ],
      "metadata": {
        "id": "5DU5Q3YbIo3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(inputs=[input_layer, decoder_input], outputs=output)\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Training data\n",
        "x_train = english_tokenizer(np.array(english_sentences))\n",
        "y_train = hinglish_tokenizer(np.array(hinglish_sentences))\n",
        "\n",
        "# Padding sequences\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, padding='post')\n",
        "y_train = keras.preprocessing.sequence.pad_sequences(y_train, padding='post')\n",
        "\n",
        "# Train the model\n",
        "model.fit([x_train, y_train[:, :-1]], y_train[:, 1:], batch_size=1, epochs=20)\n",
        "\n",
        "# Inference model (for translation)\n",
        "encoder_model = Model(inputs=input_layer, outputs=[state_h, state_c])\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_embed = keras.layers.Embedding(input_dim=len(hinglish_tokenizer.get_vocabulary()), output_dim=256)\n",
        "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
        "\n",
        "decoder_input = Input(shape=(None,))\n",
        "decoder_embedded = decoder_embed(decoder_input)\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedded, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_dense = Dense(len(hinglish_tokenizer.get_vocabulary()), activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "decoder_model = Model([decoder_input] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
      ],
      "metadata": {
        "id": "TV4nayT0JUNk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5468f1e2-16d2-4b1b-89b7-1120ad596540"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "750/750 [==============================] - 160s 209ms/step - loss: 1.0471 - accuracy: 0.8653\n",
            "Epoch 2/20\n",
            "750/750 [==============================] - 158s 210ms/step - loss: 0.8492 - accuracy: 0.8711\n",
            "Epoch 3/20\n",
            "750/750 [==============================] - 158s 210ms/step - loss: 0.7950 - accuracy: 0.8747\n",
            "Epoch 4/20\n",
            "750/750 [==============================] - 161s 214ms/step - loss: 0.7444 - accuracy: 0.8779\n",
            "Epoch 5/20\n",
            "750/750 [==============================] - 158s 211ms/step - loss: 0.6906 - accuracy: 0.8810\n",
            "Epoch 6/20\n",
            "750/750 [==============================] - 159s 211ms/step - loss: 0.6311 - accuracy: 0.8858\n",
            "Epoch 7/20\n",
            "750/750 [==============================] - 160s 213ms/step - loss: 0.5640 - accuracy: 0.8918\n",
            "Epoch 8/20\n",
            "750/750 [==============================] - 160s 213ms/step - loss: 0.4944 - accuracy: 0.8982\n",
            "Epoch 9/20\n",
            "750/750 [==============================] - 160s 213ms/step - loss: 0.4226 - accuracy: 0.9099\n",
            "Epoch 10/20\n",
            "750/750 [==============================] - 159s 212ms/step - loss: 0.3553 - accuracy: 0.9240\n",
            "Epoch 11/20\n",
            "750/750 [==============================] - 159s 212ms/step - loss: 0.2901 - accuracy: 0.9388\n",
            "Epoch 12/20\n",
            "750/750 [==============================] - 158s 210ms/step - loss: 0.2352 - accuracy: 0.9508\n",
            "Epoch 13/20\n",
            "750/750 [==============================] - 158s 210ms/step - loss: 0.1905 - accuracy: 0.9605\n",
            "Epoch 14/20\n",
            "750/750 [==============================] - 172s 230ms/step - loss: 0.1572 - accuracy: 0.9677\n",
            "Epoch 15/20\n",
            "750/750 [==============================] - 159s 211ms/step - loss: 0.1279 - accuracy: 0.9741\n",
            "Epoch 16/20\n",
            "750/750 [==============================] - 159s 212ms/step - loss: 0.1117 - accuracy: 0.9774\n",
            "Epoch 17/20\n",
            "750/750 [==============================] - 160s 214ms/step - loss: 0.0908 - accuracy: 0.9820\n",
            "Epoch 18/20\n",
            "750/750 [==============================] - 161s 214ms/step - loss: 0.0753 - accuracy: 0.9849\n",
            "Epoch 19/20\n",
            "750/750 [==============================] - 159s 211ms/step - loss: 0.0642 - accuracy: 0.9864\n",
            "Epoch 20/20\n",
            "750/750 [==============================] - 158s 211ms/step - loss: 0.0565 - accuracy: 0.9877\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating Translation Fuction"
      ],
      "metadata": {
        "id": "1U0DDN0AIwjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Translate English to Hinglish\n",
        "def translate_english_to_hinglish(input_text):\n",
        "    input_seq = english_tokenizer([input_text])\n",
        "    initial_states = encoder_model.predict(input_seq)\n",
        "    target_seq = np.zeros((1, 1))  # Initialize with start token\n",
        "\n",
        "    stop_condition = False\n",
        "    output_text = ''\n",
        "\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + initial_states)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = hinglish_tokenizer.get_vocabulary()[sampled_token_index]\n",
        "\n",
        "        if sampled_token != '<end>':\n",
        "            output_text += sampled_token + ' '\n",
        "\n",
        "        # Exit condition: either hitting max length or finding the end token\n",
        "        if sampled_token == '<end>' or len(output_text.split()) > len(input_text)+5:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1)\n",
        "        target_seq = np.array([[sampled_token_index]])\n",
        "\n",
        "        # Update states\n",
        "        initial_states = [h, c]\n",
        "\n",
        "    return output_text\n",
        "\n",
        "# Example usage\n",
        "input_text = 'Hello how are you ?'\n",
        "translated_text = translate_english_to_hinglish(input_text)\n",
        "print(translated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1M6cQWpC9NvL",
        "outputId": "053e6af8-a621-4dcb-a572-c66cbaa56907"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "actually actually actually actually friends friends friends sirf sirf sirf sirf sirf sirf bhaag barbaad barbaad bob shaark b pehle pehle gae gae referred laaye \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = 'Definitely share your feedback in the comment section.'\n",
        "translated_text = translate_english_to_hinglish(input_text)\n",
        "print(translated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5P0icAJbSVM",
        "outputId": "d60d54d3-e6ab-40ce-de5c-663490919d61"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 431ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "tim tim tim tim tim tim tim tim nihitaarth nihitaarth nihitaarth nihitaarth yek yek koriyograaphee per minions anubhav per minions anubhav parr accuracy parr kodabreking meril kheenchata ven meril ven traasadee traasadee rahi metacritic traasadee saupa kareng metacritic piece release kareng goyer sev likable pasandida kaha aya jarurat tareeke ghar victor dekhane californiamein jise bete bete kon jaega isaka kaisi \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = 'So even if its a big video, I will clearly mention all the products.'\n",
        "translated_text = translate_english_to_hinglish(input_text)\n",
        "print(translated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dut-UCujbSgW",
        "outputId": "888fb0bb-2194-4b13-f4bf-a627e7ef12d8"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "tim tim tim tim tim tim tim tim tim univerce nihitaarth nihitaarth nihitaarth nihitaarth man nihitaarth yek yek per limited low low per minions sawalbahut sawalbahut typo steven accuracy ironic anubhav zyadatar majakiya chase zyadatar sa actors gordon socho 1920 6 bhi tomaatoz position laghbhag depending remember hone khada vishwaas bad impact robin googlai classics jisne copley copley copley dekhthe apni definitely male dekhunga month paatr udaane paravaah choice chalta gru demiyan demiyan zack \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = 'I was waiting for my bag.'\n",
        "translated_text = translate_english_to_hinglish(input_text)\n",
        "print(translated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9_Vg_fsbVqD",
        "outputId": "bcdf3eaf-04e5-47a3-844e-0bdd88b06f24"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "nefario nefario shaark shaark shaark shaark shaark shaark shaark shaark shaark sirf sirf sirf sirf sirf ninteen bhaag shaark un week victor b barbaad b better charitron philmon bhaga shaadee fight \n"
          ]
        }
      ]
    }
  ]
}